---
title: "AI Safety"
collection: portfolio
date: 2025-06-01
---
<img src='/images/project_rtx.png' style='float: left; width: 100px; margin-right: 20px; height: auto;'>
<b>Time:</b> 2025 - Present <br> <b>Location:</b> Penn State University <br> <b>Advisor:</b> Prof. Rui Zhang <br> <b>Skills:</b> Group Relative Policy Optimization (GRPO) Reinforcement Learning , Uncertainty quantification (UQ), Retrieval-Augmented Generation (RAG)  <br><br> 
Currently, I am developing a safety framework that utilizes model internal signals to detect and mitigate hallucinations in Large Language Models. By leveraging Group Relative Policy Optimization (GRPO) and Reinforcement Learning, the system dynamically adjusts model outputs to align with factual correctness, aiming to create more robust and safe AI decision making agents.