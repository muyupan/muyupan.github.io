---
title: "RL for Hallucination Mitigation"
excerpt: "<img src='/images/rl.png' style='float: left; width: 100px; margin-right: 20px; height: auto;'><br><b>Time:</b> 2024 - Present <br> <b>Location:</b> Penn State University <br> <b>Advisor:</b> Prof. Rui Zhang <br> <b>Skills:</b> Python, Reinforcement Learning (RL), GRPO, LLMs <br><br> Currently, I am developing a safety framework that utilizes model internal signals to detect and mitigate hallucinations in Large Language Models. By leveraging Group Relative Policy Optimization (GRPO) and Reinforcement Learning, the system dynamically adjusts model outputs to align with factual correctness, aiming to create more robust and safe AI decision-making agents."
collection: portfolio
date: 2025-06-01
---